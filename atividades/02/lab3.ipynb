{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "# ** Análise Exploratória **\n",
    "\n",
    "#### Esse notebook introduz os conceitos de Análise Exploratória\n",
    "\n",
    "#### Para isso utilizaremos a base de dados de [Crimes de São Francisco](https://www.kaggle.com/c/sf-crime) obtidos do site de competições [Kaggle](https://www.kaggle.com/).\n",
    "\n",
    "#### ** Esse notebook contém:  **\n",
    "#### *Parte 1:* *Parsing* da base de dados de Crimes de São Francisco\n",
    "#### *Parte 2:* Estatísticas Básicas das Variáveis\n",
    "#### *Parte 3:* Plotagem de Gráficos\n",
    "\n",
    "#### Para os exercícios é aconselhável consultar a documentação da [API do PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Parte 1: Parsing da Base de Dados **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa primeira parte do notebook vamos aprender a trabalhar com arquivos CSV. Os arquivos CSV são arquivos textos representando tabelas de dados, numéricas ou categóricas, com formatação apropriada para a leitura estruturada.\n",
    "\n",
    "#### A primeira linha de um arquivo CSV é o cabeçalho, com o nome de cada coluna da tabela separados por vírgulas.\n",
    "\n",
    "#### Cada linha subsequente representa um objeto da base de dados com os valores também separados por vírgula. Esses valores podem ser numéricos, categóricos (textuais) e listas. As listas são representadas por listas de valores separadas por vírgulas e entre aspas.\n",
    "\n",
    "#### Vamos carregar a base de dados histórica de Crimes de São Francisco, um dos temas do projeto final. No primeiro passo vamos armazenar o cabeçalho em uma variável chamada `header` e imprimi-la para a descrição dos campos de nossa base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campos disponíveis: Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,X,Y\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "filename = os.path.join(\"Data\",\"Aula03\",\"Crime.csv\")\n",
    "CrimeRDD = sc.textFile(filename,8)\n",
    "header = CrimeRDD.take(1)[0] # o cabeçalho é a primeira linha do arquivo\n",
    "\n",
    "print \"Campos disponíveis: {}\".format(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durante os exercícios precisaremos pular a linha do cabeçalho de tal forma a trabalhar apenas com a tabela de dados.\n",
    "\n",
    "#### Uma forma de fazer isso é utilizando o comando `filter()` para eliminar toda linha igual a variável `header`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "CrimeHeadlessRDD = CrimeRDD.filter(lambda line: line != header)\n",
    "\n",
    "firstObject = CrimeHeadlessRDD.take(1)[0]\n",
    "print firstObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert firstObject==u'2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747', 'valor incorreto'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora temos um dataset em que cada linha é uma string contendo todos os valores. Porém, para explorarmos os dados precisamos que cada objeto seja uma lista de valores.\n",
    "\n",
    "#### Utilize o comando `split()` para transformar os objetos em listas de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2015-05-13 23:53:00', u'WARRANTS', u'WARRANT ARREST', u'Wednesday', u'NORTHERN', u'\"ARREST', u' BOOKED\"', u'OAK ST / LAGUNA ST', u'-122.425891675136', u'37.7745985956747']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "CrimeHeadlessRDD = (CrimeRDD\n",
    "                    .filter(lambda line: line != header)\n",
    "                    .map(lambda line: line.split(','))\n",
    "                    )\n",
    "\n",
    "firstObjectList = CrimeHeadlessRDD.take(1)[0]\n",
    "print firstObjectList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert firstObjectList[0]==u'2015-05-13 23:53:00', 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparem que o campo *Resolution* cujo valor no primeiro registro era \"ARREST, BOOKED\" se tornou dois campos diferentes por causa do `split()`.\n",
    "\n",
    "#### Nesses casos em que uma simples separação não funciona, nós podemos utilizar as [Expressões Regulares](http://www.rexegg.com/regex-quickstart.html). O Python tem suporte as Regex através da biblioteca `re`. Vamos utilizar o comando [`re.split()`](https://docs.python.org/2/library/re.html#re.split) para cuidar da separação de nossa base em campos.\n",
    "\n",
    "#### Além disso, vamos aproveitar para converter o primeiro campo, que representa data e hora, para objeto do tipo [`datetime`](https://docs.python.org/2/library/datetime.html) através do comando `datetime.datetime.strptime()`. Também vamos agrupar as coordenadas X e Y em uma tupla de floats.\n",
    "\n",
    "#### Outra ajuda que o Python pode nos dar é a utilização das [`namedtuple`](https://docs.python.org/2/library/collections.html#namedtuple-factory-function-for-tuples-with-named-fields) que permite acessar cada campo de cada objeto pelo nome. Ex.: rec.Dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime(Dates=datetime.datetime(2015, 5, 13, 23, 53), Category=u'WARRANTS', Descript=u'WARRANT ARREST', DayOfWeek=u'Wednesday', PdDistrict=u'NORTHERN', Resolution=[u'ARREST', u'BOOKED'], Address=u'OAK ST / LAGUNA ST', COORD=(u'-122.425891675136', u'37.7745985956747'))\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "headeritems = header.split(',') # transformar o cabeçalho em lista\n",
    "del headeritems[-1] # apagar o último item e...\n",
    "headeritems[-1] = 'COORD' # transformar em COORD\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "Crime = namedtuple('Crime',headeritems) # gera a namedtuple Crime com os campos de header\n",
    "\n",
    "REGEX = r',(?=(?:[^\"]*\"[^\"]*\")*(?![^\"]*\"))'\n",
    "# buscar por \",\" tal que após essa vírgula (?=) ou exista um par de \"\" ou não tenha \" sozinha\n",
    "# ?= indica para procurarmos pelo padrão após a vírgula\n",
    "# ?: significa para não interpretar os parênteses como captura de valores\n",
    "# [^\"]* 0 ou sequências de caracteres que não sejam aspas\n",
    "# [^\"]*\"[^\"]*\"  <qualquer caracter exceto aspas> \" <qualquer caracter exceto aspas> \"\n",
    "# ?! indica para verificar se não existe tal padrão a frente da vírgula\n",
    "\n",
    "\n",
    "def ParseCrime(rec):\n",
    "    # utilizando re.split() vamos capturar nossos valores\n",
    "    Date, Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, X, Y = re.split(REGEX,rec)\n",
    "    \n",
    "    # Converta a data para o formato datetime\n",
    "    Date = datetime.datetime.strptime(Date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # COORD é uma tupla com floats representando X e Y\n",
    "    COORD = (X,Y)\n",
    "    \n",
    "    print Resolution\n",
    "    # O campos 'Resolution' será uma lista dos valores separados por vírgula, sem as aspas\n",
    "    Resolution = Resolution.replace('\"', \"\").replace(\", \",\",\").split(',')\n",
    "    return Crime(Date, Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, COORD)\n",
    "\n",
    "# Aplique a função ParseCrime para cada objeto da base\n",
    "CrimeHeadlessRDD = (CrimeRDD\n",
    "                    .filter(lambda line: line != header)\n",
    "                    .map(lambda line: ParseCrime(line))\n",
    "                    )\n",
    "\n",
    "firstClean = CrimeHeadlessRDD.take(1)[0]\n",
    "totalRecs = CrimeHeadlessRDD.count()\n",
    "print firstClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert type(firstClean.Dates) is datetime.datetime and type(firstClean.Resolution) is list and type(firstClean.COORD) is tuple,'tipos incorretos'\n",
    "print \"OK\"\n",
    "\n",
    "assert CrimeHeadlessRDD.filter(lambda x: len(x)!=8).count()==0, 'algo deu errado!'\n",
    "print \"OK\"\n",
    "\n",
    "assert totalRecs==878049, 'total de registros incorreto'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 2: Estatísticas Básicas das Variáveis **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos aprender a filtrar a base de dados para calcular estatísticas básicas necessárias para a análise exploratória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Contagem de frequência **\n",
    "\n",
    "#### A contagem de frequência é realizada de forma similar ao exercício de contagem de palavras. Primeiro mapeamos a variável de interesse. Como exemplo vamos gerar uma lista da quantidade total de cada tipo de crime (Category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'LARCENY/THEFT', 174900), (u'OTHER OFFENSES', 126182), (u'NON-CRIMINAL', 92304), (u'ASSAULT', 76876), (u'DRUG/NARCOTIC', 53971), (u'VEHICLE THEFT', 53781), (u'VANDALISM', 44725), (u'WARRANTS', 42214), (u'BURGLARY', 36755), (u'SUSPICIOUS OCC', 31414), (u'MISSING PERSON', 25989), (u'ROBBERY', 23000), (u'FRAUD', 16679), (u'FORGERY/COUNTERFEITING', 10609), (u'SECONDARY CODES', 9985), (u'WEAPON LAWS', 8555), (u'PROSTITUTION', 7484), (u'TRESPASS', 7326), (u'STOLEN PROPERTY', 4540), (u'SEX OFFENSES FORCIBLE', 4388), (u'DISORDERLY CONDUCT', 4320), (u'DRUNKENNESS', 4280), (u'RECOVERED VEHICLE', 3138), (u'KIDNAPPING', 2341), (u'DRIVING UNDER THE INFLUENCE', 2268), (u'RUNAWAY', 1946), (u'LIQUOR LAWS', 1903), (u'ARSON', 1513), (u'LOITERING', 1225), (u'EMBEZZLEMENT', 1166), (u'SUICIDE', 508), (u'FAMILY OFFENSES', 491), (u'BAD CHECKS', 406), (u'BRIBERY', 289), (u'EXTORTION', 256), (u'SEX OFFENSES NON FORCIBLE', 148), (u'GAMBLING', 146), (u'PORNOGRAPHY/OBSCENE MAT', 22), (u'TREA', 6)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "from operator import add\n",
    "CatCountRDD = (CrimeHeadlessRDD\n",
    "               .map(lambda registry: (registry[1],1))\n",
    "               .reduceByKey(add)\n",
    "               )\n",
    "catCount = sorted(CatCountRDD.collect(), key=lambda x: -x[1])\n",
    "print catCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert catCount[0][1]==174900, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De forma similar, vamos gerar a contagem para as regiões de São Francisco (PdDistrict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'SOUTHERN', 157182), (u'MISSION', 119908), (u'NORTHERN', 105296), (u'BAYVIEW', 89431), (u'CENTRAL', 85460), (u'TENDERLOIN', 81809), (u'INGLESIDE', 78845), (u'TARAVAL', 65596), (u'PARK', 49313), (u'RICHMOND', 45209)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "RegionCountRDD = (CrimeHeadlessRDD\n",
    "                  .map(lambda registry: (registry[4],1))\n",
    "                  .reduceByKey(add)\n",
    "                 )\n",
    "regCount = sorted(RegionCountRDD.collect(), key=lambda x: -x[1])\n",
    "print regCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert regCount[0][1]==157182, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Cálculo da Média**\n",
    "\n",
    "#### Nesse exercício vamos calcular a média de crimes em cada região para cada dia da semana. Para isso, primeiro devemos calcular a quantidade de dias de cada dia da semana que existem na base de dados, para isso vamos criar uma RDD de tuplas em que o primeiro campo é a tupla da data no formato 'dia-mes-ano' e do dia da semana e o segundo campo o valor $1$.\n",
    "\n",
    "#### Em seguida, reduzimos a RDD sem efetuar a soma, mantendo o valor $1$. Essa redução filtra a RDD para que cada data apareça uma única vez. Ao final,  podemos efetuar o mapeamento de (DayOfWeek,1) e redução com soma para contabilizar quantas vezes cada dia da semana aparece na base de dados.\n",
    "\n",
    "#### Nossa próxima RDD terá como chave uma tupla ( (DayOfWeek, PdDistrict), 1) para contabilizar quantos crimes ocorreram em determinada região e naquele dia da semana. Após a redução, devemos mapear esse RDD para (DayOfWeek, (PdDistrict, contagem)).\n",
    "\n",
    "#### Finalmente, podemos juntar as duas RDDs uma vez que elas possuem a mesma chave (DayOfWeek), dessa forma teremos tuplas no formato ( DayOfWeek, ( (PdDistrict,contagem), contagemDiaDaSemana ) ). Isso deve ser mapeado para:\n",
    "\n",
    "#### ( DayOfWeek, ( PdDistrict, contagem / contagemDiaDaSemana ) )\n",
    "\n",
    "#### Lembrando de converter `contagemDiaDaSemana` para `float`. Finalmente, o resultado pode ser agrupado pela chave, gerando uma tupla ( DayOfWeek, [ (Pd1, media1), (Pd2, media2), ... ] ). Essa lista pode ser mapeada para um dicionário com o comando `dict`.\n",
    "\n",
    "#### No final, transformamos o RDD em um dicionário Python com o comando `collectAsMap()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'CENTRAL': 37.99688473520249, u'NORTHERN': 44.45794392523364, u'SOUTHERN': 64.82866043613707, u'PARK': 20.70404984423676, u'MISSION': 49.45171339563863, u'TENDERLOIN': 31.707165109034268, u'RICHMOND': 18.968847352024923, u'TARAVAL': 25.953271028037385, u'INGLESIDE': 32.230529595015575, u'BAYVIEW': 37.27414330218068}\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "from operator import add\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "\n",
    "# Lambda para converter um datetime em `Dia-Mes-Ano`\n",
    "day2str = lambda x: '{}-{}-{}'.format(x.day,x.month,x.year)\n",
    "\n",
    "totalDatesRDD = (CrimeHeadlessRDD\n",
    "                 .map(lambda registry: ((day2str(registry[0]), registry[3]), 1))\n",
    "                 .reduceByKey(lambda x,y: (x))\n",
    "                 .map(lambda registry: (registry[0][1], 1))\n",
    "                 .reduceByKey(add)\n",
    "                 )\n",
    "\n",
    "crimesWeekDayRegionRDD = (CrimeHeadlessRDD\n",
    "                           .map(lambda registry: ((registry[3], registry[4]), 1))\n",
    "                           .reduceByKey(add)\n",
    "                           .map(lambda registry: (registry[0][0], (registry[0][1], registry[1])))\n",
    "                          )\n",
    "\n",
    "RegionAvgPerDayRDD = (crimesWeekDayRegionRDD\n",
    "                      .join(totalDatesRDD)\n",
    "                      .map(lambda (day, pdCount): (day, (pdCount[0][0], pdCount[0][1] / float(pdCount[1]) )))\n",
    "                      .groupByKey()\n",
    "                      .map(lambda registry: (registry[0], dict(registry[1])))\n",
    "                     )\n",
    "\n",
    "RegionAvg = RegionAvgPerDayRDD.collectAsMap()\n",
    "print RegionAvg['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert np.round(RegionAvg['Sunday']['BAYVIEW'],2)==37.27, 'valores incorretos {}'.format(np.round(RegionAvg[0][2],2))\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Média e Desvio-Padrão pelo PySpark **\n",
    "\n",
    "#### Uma alternativa para calcular média, desvio-padrão e outros valores descritivos é utilizando os comandos internos do Spark. Para isso é necessário gerar uma RDD de listas de valores.\n",
    "\n",
    "#### Gere uma RDD contendo a tupla ( (Dates,DayOfWeek, PdDistrict), contagem), mapeie para ( (DayOfWeek,PdDistrict), Contagem) e agrupe pela chave. Isso irá gerar uma RDD ( (DayOfWeek,PdDistrict), Iterador(contagens) ).\n",
    "\n",
    "#### Agora crie um dicionário RegionAvgSpark, inicialmente vazio e colete apenas o primeiro elemento da tupla para a variável `Keys`. Itere essa variável realizando os seguintes passos:\n",
    "\n",
    "* #### Se `key[0]` não existir no dicionário, crie a entrada `key[0]` como um dicionário vazio.\n",
    "* #### Mapeie countWeekDayDistRDD filtrando por `key` e gerando a RDD com os valores da tupla. Note que não queremos uma lista de listas.\n",
    "* #### Insira a tupla (media, desvio-padrão) utilizando os comandos `mean()` e `stdev()` do PySpark, armazenando na chave RegionAvgSpark[ key[0] ][ key[1] ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1be9e8463dc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mRegionAvgSpark\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mRegionAvgSpark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mRegionAvgSpark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlistRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mRegionAvgSpark\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sunday'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mstdev\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[1;36m0.816\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m         \"\"\"\n\u001b[1;32m-> 1206\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msampleStdev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mstats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda2\\lib\\socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "countWeekDayDistRDD = (CrimeHeadlessRDD\n",
    "                       .map(lambda registry: ((registry[0], registry[3], registry[4]),1))\n",
    "                       .reduceByKey(add)\n",
    "                       .map(lambda registry: ((registry[0][1], registry[0][2]), registry[1]))\n",
    "                       .groupByKey()\n",
    "                       )\n",
    "\n",
    "# Esse procedimento só é viável se existirem poucas chaves <- em minha máquina não terminou de rodar em ~15min\n",
    "RegionAvgSpark = {}\n",
    "Keys = countWeekDayDistRDD.map(lambda rec: rec[0]).collect()\n",
    "for key in Keys:\n",
    "    listRDD = (countWeekDayDistRDD\n",
    "               .filter(lambda rec: rec[0]==key)\n",
    "               .flatMap(lambda rec: rec[1])\n",
    "               )\n",
    "    if key[0] not in RegionAvgSpark:\n",
    "        RegionAvgSpark[key[0]] = {}    \n",
    "    RegionAvgSpark[key[0]][key[1]] = (listRDD.mean(), listRDD.stdev())\n",
    "    \n",
    "print RegionAvgSpark['Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(RegionAvgSpark['Sunday']['BAYVIEW'][0],2)==37.39 and np.round(RegionAvgSpark['Sunday']['BAYVIEW'][1],2)==10.06, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: Plotagem de Gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos aprender a manipular os dados para gerar listas de valores a serem utilizados na plotagem de gráficos.\n",
    "\n",
    "#### Para a plotagem de gráficos vamos utilizar o [`matplotlib`](http://matplotlib.org/) que já vem por padrão na maioria das distribuições do Python (ex.: Anaconda). Outras bibliotecas alternativas interessantes são: [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) e [Bokeh](http://bokeh.pydata.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Gráfico de Barras **\n",
    "\n",
    "#### O gráfico de barras é utilizado quando queremos comparar dados entre categorias diferentes de uma variável categórica. Como exemplo, vamos contabilizar o número médio de crimes diários por região.\n",
    "\n",
    "#### Vamos primeiro criar a RDD totalDatesRDD que contém a lista de dias únicos, computaremos o total de dias com o comando `count()` armazenando na variável `totalDays`. Não se esqueça de converter o valor para `float`.\n",
    "\n",
    "#### Em seguida, crie o RDD avgCrimesRegionRDD que utiliza a RDD RegionCountRDD para calcular a média de crimes por região.\n",
    "\n",
    "#### Utilizando o comando `zip()` do Python é possível descompactar um dicionário em duas variáveis, uma com as chaves e outra com os valores. Utilizaremos essas variáveis para a plotagem do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,COORD\n",
    "\n",
    "# Lambda para converter um datetime em `Dia-Mes-Ano`\n",
    "day2str = lambda x: '{}-{}-{}'.format(x.day,x.month,x.year)\n",
    "\n",
    "totalDatesRDD = (CrimeHeadlessRDD\n",
    "                 .map(lambda rec: (day2str(rec.Dates),1))\n",
    "                 .reduceByKey(lambda x,y: x)\n",
    "                 )\n",
    "\n",
    "totalDays = float(totalDatesRDD.count())\n",
    "\n",
    "avgCrimesRegionRDD = (RegionCountRDD\n",
    "                      .map(lambda rec: (rec[0],rec[1]/totalDays))\n",
    "                     )\n",
    "\n",
    "Xticks,Y = zip(*avgCrimesRegionRDD.collectAsMap().items())\n",
    "indices = np.arange(len(Xticks))\n",
    "width = 0.35\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.bar(indices,Y, width)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xticks(indices+width/2., Xticks, rotation=17 )\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.xlabel('Region')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quando temos subcategorias de interesse, podemos plotar através de um gráfico de barras empilhado. Vamos plotar o conteúdo da variável RegionAvg.\n",
    "\n",
    "#### Primeiro passo é criar um dicionário `Y` em que a chave é o dia da semana e o valor é uma `np.array` contendo a média de cada região para aquele dia.\n",
    "\n",
    "#### Em seguida precisamos criar uma matriz `Bottom` que determina qual é o início de cada uma das barras. O início da barra do dia `i` deve ser o final da barra do dia `i-1`.\n",
    "\n",
    "#### Com isso calculado podemos gerar um plot por dia com o parâmetro bottom correspondente ao vetor `Bottom` daquele dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dias da semana como referência\n",
    "Day = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "\n",
    "# Uma cor para cada dia\n",
    "Color = ['r','b','g','y','c','k','purple']\n",
    "\n",
    "# Dicionário (dia, array de médias)\n",
    "Y = {}\n",
    "for day in Day:\n",
    "    Y[day] = np.array([RegionAvg[day][x] for x in Xticks])\n",
    "\n",
    "# Matriz dias x regiões    \n",
    "Bottom = np.zeros( (len(Day),len(Xticks)) )\n",
    "for i in range(1,len(Day)):\n",
    "    Bottom[i,:] = Bottom[i-1,:]+Y[Day[i-1]]\n",
    "    \n",
    "indices = np.arange(len(Xticks))\n",
    "width = 0.35\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "\n",
    "# Gera uma lista de plots, um para cada dia\n",
    "plots = [plt.bar(indices,Y[Day[i]], width, color=Color[i], bottom=Bottom[i]) for i in range(len(Day))]\n",
    "\n",
    "plt.legend( [p[0] for p in plots], Day,loc='center left', bbox_to_anchor=(1, 0.5) ) \n",
    "    \n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xticks(indices+width/2., Xticks, rotation=17 )\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.xlabel('Region')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Gráfico de Linha **\n",
    "\n",
    "#### O gráfico de linha é utilizado principalmente para mostrar uma tendência temporal.\n",
    "\n",
    "#### Nesse exercício vamos primeiro gerar o número médio de crimes em cada hora do dia.\n",
    "\n",
    "#### Primeiro, novamente, geramos um RDD contendo um único registro de cada hora para cada dia. Em seguida, contabilizamos a soma da quantidade de crime em cada hora. Finalmente, juntamos as duas RDDs e calculamos a média dos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CrimeHeadlessRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2dd0ab723ddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparseWeekday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'{}-{}-{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m hoursRDD = (CrimeHeadlessRDD\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparseWeekday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregistry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CrimeHeadlessRDD' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "parseWeekday = lambda x: '{}-{}-{}'.format(x.day, x.month, x.year)\n",
    "\n",
    "hoursRDD = (CrimeHeadlessRDD\n",
    "            .map(lambda registry: ((parseWeekday(registry[0]), registry[0].hour), 1))\n",
    "            .reduceByKey(lambda x,y: x)\n",
    "            .map(lambda registry: (registry[0][1], 1))\n",
    "            .reduceByKey(add)\n",
    "           )\n",
    "\n",
    "\n",
    "#print hoursRDD.filter(lambda x: x[0][0] =='17-5-2010').collect()\n",
    "\n",
    "crimePerHourRDD = (CrimeHeadlessRDD\n",
    "                   .map(lambda registry: ((parseWeekday(registry[0]), registry[0].hour), 1))\n",
    "                   .reduceByKey(add)\n",
    "                  )\n",
    "\n",
    "avgCrimeHourRDD = (crimePerHourRDD\n",
    "                   .join(hoursRDD)\n",
    "                   .map(lambda (reg, count): (hour, float(crimeCount[1]) / totalHours))\n",
    "                  )\n",
    "print avgCrimeHourRDD.take(10)\n",
    "\n",
    "\n",
    "#crimePerHour = avgCrimeHourRDD.collect()\n",
    "#print crimePerHour[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crimePerHour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-35f4f4b18cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrimePerHour\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m19.96\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valores incorretos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"OK\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crimePerHour' is not defined"
     ]
    }
   ],
   "source": [
    "assert np.round(crimePerHour[0][1],2)==19.96, 'valores incorretos'\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crimePerHourSort = sorted(crimePerHour,key=lambda x: x[0])\n",
    "\n",
    "X,Y = zip(*crimePerHourSort)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.plot(X,Y)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.ylabel('Avg. Number of crimes')\n",
    "plt.xlabel('Hour')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Gráfico de Dispersão**\n",
    "\n",
    "#### O gráfico de dispersão é utilizado para visualizar correlações entre as variáveis. Com esse gráfico é possível observar se o crescimento da quantidade de uma categoria está relacionada ao crescimento/decrescimento de outra (mas não podemos dizer se uma causa a outra).\n",
    "\n",
    "#### Na primeira parte do exercício calcularemos a correlação entre os diferentes tipos de crime. Para isso primeiro precisamos construir uma RDD em que cada registro corresponde a uma data o valor contido nele é a quantidade de crimes de cada tipo.\n",
    "\n",
    "#### Diferente dos exercícios anteriores, devemos manter essa informação como uma lista de valores em que todos os registros sigam a mesma ordem da lista de crimes.\n",
    "\n",
    "#### O primeiro passo é criar uma RDD com a tupla ( (Mes-Ano, Crime), 1 ) e utilizá-la para gerar a tupla ( (Mes-Ano,Crime) Quantidade ).\n",
    "\n",
    "#### Mapeamos essa RDD para definir Mes-Ano como chave e agrupamos em torno dessa chave, gerando uma lista de quantidade de crimes em cada data. Aplicamos a função `dict()` nessa lista para obtermos uma RDD no seguinte formato: (Mes-Ano, {CRIME: quantidade}).\n",
    "\n",
    "#### Além disso, vamos criar a variável `crimes` contendo a lista de crimes contidas na lista de pares `catCount` computada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2-2007', {u'KIDNAPPING': 12, u'WEAPON LAWS': 49, u'SECONDARY CODES': 61, u'WARRANTS': 296, u'LOITERING': 4, u'EMBEZZLEMENT': 13, u'FRAUD': 87, u'DRIVING UNDER THE INFLUENCE': 13, u'VEHICLE THEFT': 212, u'ROBBERY': 135, u'BURGLARY': 192, u'SUSPICIOUS OCC': 207, u'RECOVERED VEHICLE': 30, u'ASSAULT': 456, u'FORGERY/COUNTERFEITING': 81, u'BAD CHECKS': 3, u'DRUNKENNESS': 24, u'GAMBLING': 2, u'OTHER OFFENSES': 757, u'SUICIDE': 3, u'ARSON': 5, u'DRUG/NARCOTIC': 504, u'SEX OFFENSES NON FORCIBLE': 2, u'PROSTITUTION': 76, u'VANDALISM': 237, u'NON-CRIMINAL': 490, u'LIQUOR LAWS': 17, u'TRESPASS': 50, u'SEX OFFENSES FORCIBLE': 23, u'STOLEN PROPERTY': 19, u'BRIBERY': 3, u'FAMILY OFFENSES': 4, u'MISSING PERSON': 154, u'DISORDERLY CONDUCT': 24, u'RUNAWAY': 13, u'LARCENY/THEFT': 835})]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "parseMonthYear = lambda x: '{}-{}'.format(x.month, x.year)\n",
    "\n",
    "crimes = map(lambda x: x[0], catCount)\n",
    "\n",
    "datesCrimesRDD = (CrimeHeadlessRDD\n",
    "                  .map(lambda registry: ((parseMonthYear(registry[0]), registry[1]),1))\n",
    "                  .reduceByKey(add)\n",
    "                  .map(lambda registry: (registry[0][0],(registry[0][1], registry[1])))\n",
    "                  .groupByKey()\n",
    "                  .map(lambda registry: (registry[0], dict(registry[1])))\n",
    "                  .cache()\n",
    "                 )\n",
    "\n",
    "print datesCrimesRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "assert datesCrimesRDD.take(1)[0][1][u'KIDNAPPING']==12,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O próximo passo consiste em calcular o total de pares Mes-Ano para ser possível o cálculo da média.\n",
    "\n",
    "#### Finalmente, criamos a RDD `fractionCrimesDateRDD` em que a chave é Mes-Ano e o valor é uma lista da fração de cada tipo de crime ocorridos naquele mês e ano. Para gerar essa lista vamos utilizar o *list comprehension* do Python de tal forma a calcular a fração para cada crime na variável `crimes`.\n",
    "\n",
    "#### Os dicionários em Python tem um método chamado `get()` que permite atribuir um valor padrão caso a chave não exista. Ex.: `dicionario.get( chave, 0.0)` retornará 0.0 caso a chave não exista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "totalPerDateRDD = (CrimeHeadlessRDD\n",
    "                   .map(lambda registry: (parseMonthYear(registry[0]),1))\n",
    "                   .reduceByKey(add)\n",
    "                  )\n",
    "\n",
    "fractionCrimesDateRDD = (datesCrimesRDD\n",
    "                         .map(lambda registry: (parseMonthYear(registry[0]), )\n",
    "                         .<COMPLETAR>\n",
    "                         .cache()\n",
    "                        )\n",
    "\n",
    "print fractionCrimesDateRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(fractionCrimesDateRDD.take(1)[0][1][0][1]-0.163950)<1e-6,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalmente, utilizaremos a função `Statistics.corr()` da biblioteca [`pyspark.mlllib.stat`](https://spark.apache.org/docs/1.1.0/api/python/pyspark.mllib.stat.Statistics-class.html).\n",
    "\n",
    "#### Para isso mapeamos nossa RDD para conter apenas a lista de valores da lista de tuplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "corr = Statistics.corr(fractionCrimesDateRDD.map(lambda rec: map(lambda x: x[1],rec[1])))\n",
    "print corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertendo a matriz `corr` para `np.array` podemos buscar pelo maior valor negativo e positivo diferentes de 1.0. Para isso vamos utilizar as funções `min()` e `argmin()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npCorr = np.array(corr)\n",
    "rowMin = npCorr.min(axis=1).argmin()\n",
    "colMin = npCorr[rowMin,:].argmin()\n",
    "print crimes[rowMin], crimes[colMin], npCorr[rowMin,colMin]\n",
    "\n",
    "npCorr[npCorr==1.] = 0.\n",
    "rowMax = npCorr.max(axis=1).argmax()\n",
    "colMax = npCorr[rowMax,:].argmax()\n",
    "print crimes[rowMax], crimes[colMax], npCorr[rowMax,colMax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora que sabemos quais crimes tem maior correlação, vamos plotar um gráfico de dispersão daqueles com maior correlação negativa.\n",
    "\n",
    "#### Primeiro criamos duas RDDs, `var1RDD` e `var2RDD`. Elas são um mapeamento da `fractionCrimesDateRDD` filtradas para conter apenas o crime contido em Xlabel e Ylabel, respectivamente.\n",
    "\n",
    "#### Juntamos as duas RDDs em uma única RDD, `correlationRDD` que mapeará para tuplas de valores, onde os valores são as médias calculadas em fractionCrimesDateRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "Xlabel = 'FORGERY/COUNTERFEITING'#'DRIVING UNDER THE INFLUENCE'\n",
    "Ylabel = 'NON-CRIMINAL'#'LIQUOR LAWS'\n",
    "\n",
    "\n",
    "\n",
    "var1RDD = (fractionCrimesDateRDD\n",
    "           .map(lambda rec: (rec[0], filter(lambda x: x[0]==Xlabel,rec[1])[0][1]))\n",
    "          )\n",
    "var2RDD = (fractionCrimesDateRDD\n",
    "           .map(lambda rec: (rec[0], filter(lambda x: x[0]==Ylabel,rec[1])[0][1]))\n",
    "          )\n",
    "\n",
    "correlationRDD = (var1RDD\n",
    "                  .<COMPLETAR>\n",
    "                  .<COMPLETAR>\n",
    "                 )\n",
    "\n",
    "\n",
    "Data = correlationRDD.collect()\n",
    "print Data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(Data[0][0]-0.015904)<1e-6, 'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No gráfico abaixo, é possível perceber que quanto mais crimes do tipo *NON-CRIMINAL* ocorrem em um dia, menos *FORGERY/COUNTERFEITING* ocorrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = zip(*Data)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.scatter(X,Y)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xlabel(Xlabel)\n",
    "plt.ylabel(Ylabel)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3d) Histograma **\n",
    "\n",
    "#### O uso do Histograma é para visualizar a distribuição dos dados. Dois tipos de distribuição que são observadas normalmente é a Gaussiana, em que os valores se concentram em torno de uma média e a Lei de Potência, em que os valores menores são observados com maior frequência.\n",
    "\n",
    "#### Vamos verificar a distribuição das prisões efetuadas (categoria *ARREST* em * Resolution*) em cada mês. Com essa distribuição poderemos verificar se o número de prisões é consistente durante os meses do período estudado.\n",
    "\n",
    "#### Primeiro criaremos uma RDD chamada `bookedRDD` que contém apenas os registros contendo *ARREST* no campo *Resolution* (lembre-se que esse campo é uma lista) e contabilizar a quantidade de registros em cada 'Mes-Ano'. Ao final, vamos mapear para uma RDD contendo apenas os valores contabilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1914, 2543, 1909, 1866, 2235]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "bookedRDD = (CrimeHeadlessRDD\n",
    "             .filter(lambda registry: u'ARREST' in registry[5])\n",
    "             .map(lambda registry: (parseMonthYear(registry[0]), 1))\n",
    "             .reduceByKey(add)\n",
    "             .map(lambda registry: registry[1])\n",
    "            )\n",
    "\n",
    "Data = bookedRDD.collect()\n",
    "print Data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "assert Data[0]==1914,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.hist(Data)\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.xlabel('ARRESTED')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notem que lemos o histograma da seguinte maneira: em cerca de 50 meses foram observadas entre 1750 e 2000 prisões. Porém, não sabemos precisar em quais meses houve um aumento ou redução das prisões. Isso deve ser observado através de um gráfico de linha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3e) Box-plot**\n",
    "\n",
    "#### O Box-plot é um gráfico muito utilizado em estatística para visualizar o resumo estatístico de uma variável.\n",
    "\n",
    "#### Para esse exercício vamos plotar duas box-plot sobre a média do número de prisões durante os meses analisados para os crimes do tipo *ROBBERY* e *ASSAULT*.\n",
    "\n",
    "#### O mapeamento é exatamente o mesmo do exercício anterior, porém filtrando para o tipo de roubo analisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "\n",
    "parseDayMonth = lambda x: '{}-{}'.format(x.month,x.year)\n",
    "\n",
    "#está exatamente igual ao exercício anterior, filtrando a categoria para robbery, mas valores não batem com assert.\n",
    "robberyBookedRDD = (CrimeHeadlessRDD\n",
    "                     .filter(lambda registry: u'ROBBERY' == registry[1])\n",
    "                     .map(lambda registry: (parseMonthYear(registry[0]), 1))\n",
    "                     .reduceByKey(add)\n",
    "                     .map(lambda registry: registry[1])\n",
    "                    )\n",
    "\n",
    "assaultBookedRDD = (CrimeHeadlessRDD\n",
    "                     .filter(lambda registry: u'ASSAULT' == registry[1])\n",
    "                     .map(lambda registry: (parseMonthYear(registry[0]), 1))\n",
    "                     .reduceByKey(add)\n",
    "                     .map(lambda registry: registry[1])\n",
    "                    )\n",
    "\n",
    "robData = robberyBookedRDD.collect()\n",
    "assData = assaultBookedRDD.collect()\n",
    "print robData[0]\n",
    "print assData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "valores incorretos",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7c6640a96da2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mrobData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'valores incorretos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'ok'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0massData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m152\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'valores incorretos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'ok'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: valores incorretos"
     ]
    }
   ],
   "source": [
    "assert robData[0]==27,'valores incorretos'\n",
    "print 'ok'\n",
    "assert assData[0]==152,'valores incorretos'\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No gráfico abaixo, percebemos que existem, em média, muito mais prisões para o tipo *ASSAULT* do que o tipo *ROBBERY*, ambos com pequena variação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
    "plt.boxplot([robData,assData])\n",
    "plt.grid(b=True, which='major', axis='y')\n",
    "plt.ylabel('ARRESTED')\n",
    "plt.xticks([1,2], ['ROBBERY','ASSAULT'])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
